---
title: "Name: Xiaohan Su"
author: 
- |
    | Student number: 21188261
date: "`r format(Sys.time(), '%X, %d %B, %Y')`"
output: html_document
---
# hello
# Originality declaration

I, [**Xiaohan Su**], confirm that the work presented in this assessment is my own. Where information has been derived from other sources, I confirm that this has been indicated in the work.

date: `r format(Sys.time(), '%d %B, %Y')`

# Initial project scope

* What is my research question - is it different to the set question and why

My question is "are the evictions in 2020 for New York spatially random or do they exhibit clustering"

* Is it appropriate to use 2020 as a year?

I think it is appropriate to use 2020 as a year because it is a recent year, the result may be more relevant for today. This research will identify spatial patterns that can be used to inform future work on spatial factors on New York evicitions. 

The null hypothesis that I am going to make is that there is no pattern of evictions in 2020 of New York City.

* Data
  * What do i have
  I have two data. The first one is list of evictions in New York from 2017 to present. The second one is GIS data: Boudaries of Community Districts for New York City clipped to the shoreline.  
  * What does it contain
  The dataset is about pending, scheduled and executed evictions within the five boroughs of NYC from 2017 to present, it contains information about Court Index Number, Docket Number, Eviction Address, Apartment Number, Executed Date, Marshal First Name, Marshal Last Name, Residential or Commercial (property type), Borough, Zip Code and Scheduled Status (Pending/Scheduled). 
  * What are the NA values - do they matter
  the NA values in the csv file are empty spaces, I think they will matter in my later analysis because they will affect the data type of some columns that contain these entries, and might impact my later calculation and the analysis result if I am going to use those columns. So I need to drop these empty spaces as NAs. 
  * Who collected the data - will they have any bias (e.g. remember gerrymandering / using data for a )
  Data on evictions is collected d from most of the New York City Marshals. Marshals are impartial public employees, so I don't think they have any bias towards the data, it is rather trustworthy and reliable.
  * Is there any accuracy information associated with the data - probably not, because it is an official data provided by Department of Investigation and is publicly available on NYC OpenData. So inaccuracies in the eviction records may be rare and can be solved immediately once being discovered. 
  * What is the CRS - is that useful
  The local coordinate system known as the New York State Plane Long Island Zone (EPSG 2263) offers a high level of precision and strikes a good balance between size and shape.
  * Do i need anything else or what might be useful
  I think I need some reports or analysis or some key summaries about evictions in New York City for the recent years, so that I know where to focus on in this dataset if I am asked to analyse their data and figure out some solutions about preventing people from being evicted in the future. 

* How will i wrangle the data (based on the previous points) to apply the methods

Based on the previous points, the first thing to do when I wrangle the data is that I need to drop all the NA values in the list, that is all the empty entries. Then I will open my data in EXCEL to get myself familiar with it, to discovery some key fields, and try to understand more about my data. After that is to organize the data, because some data may not be useful for my analysis, the computation and analysis in the following phases will be made simpler by completing this step. The next step is to clean my data, for instance, we need to identify dates that have been formatted differenty, and to eliminate outliers that would skew results and format null values. This process is crucial for guaranteeing the overall quality of our data. Next step is to enrich my data, at this stage, I would decide if my data set would benefit from additional data that could be easily added. 

* What are the limitations and assumptions (of either the data or the analysis)

The limitations of my analysis are my initial project scope is rather rough because I have not looked deeply into the data to come up with some constructive questions. so the following analysis may or may not show the results that I want to see, or the outcomes might not be so interesting to explore. But this can be regarded as my initial attempt for my further analysis, I might come up with more ideas that worth analysing while exploring my data. 

Some assumptions that I have made of the data are that it is complete, accurate and without any bias, so it can be used to analyse evictions pattern in New York for the recent years. 
In term of assumptions made for the analysis, I assume that the analysis can be conducted using some methods of point pattern analysis that I have learned in this semester, such as ripley's k, dbscan, to look for the spatial pattern of evictions in my study area. Also some data cleaning and manipulation techniques that I have learned can be useful when conducting my analysis. 

```{r}
library(tidyverse)
library(sf)
library(tmap)
library(janitor)
library(spatstat)
```

## Data loading

Before reading spatial data into R, I open it with QGIS to check this data. It looks great. I also open "Evictions.csv" with Excel, and I found some entries are empty.

Now I am going to read in the data in R. Note that when reading the csv, we need to drop the space using na=" ".

```{r}
evictions_points <- read_csv("/Users/xiaohan/Downloads/casa0005-practice-exam-2022-23-xiaohansu59/data/Evictions.csv", na=" ")

community_areas <- st_read(here::here("data", 
                                      "Community Districts",
                                      "geo_export_69c9b71a-e4d9-4bfb-b84b-3c926e6bf726.shp"))
```

Check that our data has been read in correctly, for example, numeric data haven't been read in as text or other variables.

```{r}
Datatypelist <- evictions_points %>% 
  summarise_all(class) %>%
  pivot_longer(everything(), 
               names_to="All_variables", 
               values_to="Variable_class")

Datatypelist
```

## Data wrangling

Check the coordinates on this website for the csv - <https://www.latlong.net/>. Looks like the are in WGS84. Convert csv to sf object for mapping. Missing values for coordinates thrown an error so i need to filter them out.

```{r}
points <- evictions_points%>%
  filter(Longitude<0 & Latitude>0)%>%
  
  st_as_sf(., coords = c("Longitude", "Latitude"), 
           crs = 4326)
```

64,653 features now from 71,522 in the original dataset.

Now I want to make a map, because I want to see how those points are distributed and their densities in different locations, and I want to see if they are all within the boundaries.

```{r}
tmap_mode("plot")
tm_shape(community_areas) +
  tm_polygons(col = NA, alpha = 0.5) +
  tm_shape(points) +
  tm_dots(col = "blue")
```

A lot of points! I can see that they are dense in the mojority of our plot, especially in the middle and the upper part on the map, but I can't tell if they are all within the boundaries from this plot, so I need to use a spatial subset to check and select only the points that are within the boundaries.

Here noted that if I don't transform the coordinates of the community areas, it will throw an error saying that st_crs(x) == st_crs(y) is not TRUE, so it means the CRSs of the data don't match. I think the error is just how they are set in the data, but anyway, i will transform the coordinates of the community areas to make both CRSs the same.

```{r}
community_areas <- community_areas%>%
  st_transform(., 4326)

points_sub <- points[community_areas,]
```

Still have 64,653 points as before doing the spatial subsetting, so all points were already intersecting the boundaries. 

Ok, so now let's move on to our question. I want to focus on 2020 and eviction (not legal possession) and the property type that I am interested in doing analysis is Residential (not commercial), let's reduce our data to that.

EXPLAIN...I have used string detect here to find the rows that i 2020 within the column executed_date - what might be the issues with this, why have i made this assumption...

Noted that I have used str_detect here to find the rows that are 2020 within the column executed_date. But we might have some issues with this, because this column is of data type Floating Timestamp, it means the date that evictions have been executed, it is not of data type Plain Text as some other columns. So here I just make R to regard the data type of this column as string in order to use string detect. So my assumption here is that string detect could also work with the data type of Floating Timestamp. 

```{r}
points_sub_2020<-points_sub%>%
  clean_names()%>%
  filter(str_detect(executed_date, "2020"))%>%
  # filter(eviction_legal_possession=="Eviction")%>%
  filter(residential_commercial=="Residential")
```

This has reduced it to 360 points. Maybe the amount of points is too small for my later clustering analysis? So I think I can remove the legal possession/eviction line, then the number of points I get is 2,859, I think in this case the size of my selected data is proper for the later part of point pattern analysis that focuses on some clusters or dispersed patterns of evictions. 

Now let's make a map for our chosen points to see what they look like.

```{r}
tmap_mode("plot")
tm_shape(community_areas) +
  tm_polygons(col = NA, alpha = 0.5) +
tm_shape(points_sub_2020) +
  tm_dots(col = "blue")
```

## Data analysis

Let's do some point pattern analysis.

First, I need to change my coordinates because if I don't do that, I will get an error message when I creating a window, saying 'Only projected coordinates may be converted to spatstat class object'. So let's project to EPSG 6538, as it uses meters, which is a better one than EPSG 2263, because this is in feet. 

```{r}
community_areas_projected <- community_areas %>%
  st_transform(., 6538)

points_sub_2020_projected <- points_sub_2020 %>%
  st_transform(., 6538)


window <- as.owin(community_areas_projected)
plot(window)

#create a sp object
points_sub_2020_projected_sp<- points_sub_2020_projected %>%
  as(., 'Spatial')
#create a ppp object
points_sub_2020_projected_sp.ppp <- ppp(x=points_sub_2020_projected_sp@coords[,1],
                          y=points_sub_2020_projected_sp@coords[,2],
                          window=window)
```

Now I am goint to use Ripley's K function because I want to know if I have spatial clusters in the data.

The Ripley's K function compares the observed point distribution with the Poisson random model for a variety of various distance radii. Specifically, the Kpois(r) line in Red represents the theoretical value of K for each distance window (r), which indicates the Poisson assumption of Complete Spatial Randomness. The black line represents the estimated K values that account for the impact of the study area's edge. The data seem to be clustered at the distance where the value of K is above the red line, and the data are scattered when the value of K is below the line. 

The results of Ripley's K indicate that we have clusters at all distances because the values of K are above the red line all the time, meaning that there are more points than predicted that are close to one another. So the process is not CSR and that the points are more clustered than expected. 

```{r}
K <- points_sub_2020_projected_sp.ppp %>%
  Kest(., correction="border") %>%
  plot()
```

Then is the DBSCAN analysis

EXPLAIN...why i am using DBSCAN, what does it show: 

The reason that I am using DBSCAN is that I want to know where in our data the clusters are occuring, because Ripley's K analysis can only help us detect if we have spatial clusters in our point data, but it cannot tell us where those clusters are located in space. DBSCAN is an alternative method that allows us to learn where the clusters are located in space. 

The results of DBSCAN analysis show that there are five clusters in the area I am analysing for these values of eps and MinPts. 

Why did i select the values of eps and minpts 

* How many evictions do we need for a cluster
* How far must they be

In the DBSCAN analysis, we need to enter two arguments. The first parameter is Epsilon, which is the neighbourhood radius around a point x, or sometimes referred to as the x-neighbourhood. The second parameter is MinPts, which is the minimum number of points required to qualify as a cluster. 

The reason for me to select the values of eps and minpts relates to two aspects: Firstly, I need to consider what number of evictions are required for a cluster. Secondly, how far the different eviction clusters in the state are from each other. Furthermore, based on the findings of the prior Ripley's K analysis, we can observe that clustering occurs from the beginning of zero distance, so the result is not helpful for me to select a proper eps value as we don't know the difference between clustering, random, and dispersed distributions when considering the x-neighbourhood for a cluster (the eps parameter). 

Ripley's K suggests a higher eps, but doesn't consider the min points. To find the number of samples (MinPts) that provides the best fit, some testing is necessary. I tried a few values and these seemed to give a reasonable result - it is a limitation and other methods (HDBSCAN) can overcome it. 

Based on the above considerations, I decide to choose eps = 1000 and MinPts = 50 as my initial attempts for DBSCAN method. Now I will use 1000m as a starting point for eps value and look for clusters of at least 50 points. 


* I used the distplot in the code below - EXPLAIN what distplot does and shows...

The kNNdistplot() function from the dbscan package can be used to determine an appropriate eps value based on the plot's "knee". On this plot, I can see the average distance between each point and its k nearest neighbours displayed in an ascending order. A knee is a point on the k-distance curve where a sharp change takes place. 

The distplot shows that the optimal eps value is around a distance of 3,000 as this is where a sharp change takes place on the k-distance curve. 

```{r}
library(sp)

#first extract the points from the spatial points data frame
points_todf <- points_sub_2020_projected_sp %>%
  coordinates(.)%>%
  as.data.frame()

#now run the dbscan analysis
points_todf_DBSCAN <- points_todf %>%
  fpc::dbscan(.,eps = 1000, MinPts = 50)

points_todf%>%
  dbscan::kNNdistplot(.,k=50)

#now quickly plot the results
plot(points_todf_DBSCAN, points_todf, main = "DBSCAN Output", frame = F)
plot(community_areas_projected$geometry, add=T)
```

Add the cluster information to our original dataframe

```{r}
points_todf<- points_todf %>%
  mutate(dbcluster=points_todf_DBSCAN$cluster)
```

Convert our original data frame to a sf object again.

Since 0 means all points that aren't in a cluster, it isn't actually a cluster, we need to remove it from the dataframe.

```{r}
tosf <- points_todf%>%
  st_as_sf(., coords = c("coords.x1", "coords.x2"), 
                   crs = 6538)%>%
  filter(dbcluster>0)
```

Map the data - remember we are adding layers one by one

```{r}
ggplot(data = community_areas_projected) +
  # add the geometry of the community areas
  geom_sf() +
  # add the geometry of the points - i have had to set the data here to add the layer
  geom_sf(data = tosf, size = 0.4, colour=tosf$dbcluster, fill=tosf$dbcluster)
```

We can also make a thematic map

```{r}
library(tmap)
library(sf)

#tmaptools::palette_explorer()
library(RColorBrewer)
library(tmaptools)
colours<- get_brewer_pal("Set1", n = 19)

tmap_mode("plot")
tm_shape(community_areas) +
  tm_polygons(col = NA, alpha = 0.5) +
tm_shape(tosf) +
  tm_dots(col = "dbcluster",  palette = colours, style = "cat")
```

## Reflection

from the above analysis, we can see that there are five distinct clusters of residential evictions in New York City for 2020. These clusters reveal a pattern of eviction hotspots across the city, with certain neighbourhoods experiencing significantly higher rates of eviction than others. This information is important for understanding the current state of housing insecurity in New York City, and for developing targeted strategies to reduce evictions and improve housing security for vulnerable populations. 

There are many potential factors that could influence the rate of evictions in a given area. Some possible factors include the overall economic health of the area, the availability and affordability of housing, the strength of tenant protection laws, and the level of demand for housing. Additionally, demographic factors such as the composition of the population, the level of education and income among residents, and the presence of vulnerable groups such as children or the elderly may also play a role in determining the rate of evictions. 

Now I have found that there are five clusters of residential evictions in New York for the year 2020, I can extract the community districts and then look at some other data (e.g. census data) to explore factors that might influence evictions. I could also compare different years, for example, comparing the eviction clusters in year 2019, 2020 and 2021, to see if the clusters are in different parts of the city. It is possible that eviction clusters could be located in different parts of the city in different years due to a variety of factors. For example, the economic health of different areas of the city could vary over time, leading to differences in the rate of evictions. Additionally, changes in housing availability or affordability, tenant protection laws, and so on. COVID-19 is also another important factor related to eviction issue. The economic disruption caused by the pandemic has led to widespread job losses and reductions in income for many individuals and families, making it difficult for them to pay their rent or mortgage. As a result, there has been a sharp increase in the number of evictions in many cities.



